\section{Optimizations}
\label{sec:opt}

Maximizing Spinless Traversal's advantage
  over Double Dirty Bit
  requires careful optimizations
  to reduce work needed, improve cache locality,
  reduce memory traffic,
  and alleviate branch mispredictions.
Unlike the optimization passes of the Megatron compiler,
  which apply to all invalidation algorithms,
  these optimizations are implemented
  in the Spineless Traversal invalidation algorithm
  and only affect it.

\subsection{Queue Compression}

Although the queue size is typically small, often only tens of elements long, it can occasionally become very large. This typically happens when the entire page must be re-laid-out, such as when the browser window is resized. In this case, every dirty bit ends up in the priority queue, and the queue can grow to thousands of elements long. This not only means that more queue pushes and pops are needed, but also that those pushes and pops take longer.

In theses cases the queue is often dense: for every dirty bit in the queue, its successor is also likely dirty and enqueued as well. Leveraging this insight, we can shrink the queue by representing sequences of enqueued dirty bits implicitly. That is, every time a queue element is popped, Megatron not only recomputes those fields but it also checks if the succeeding dirty bit is dirty and, if so, recompute it too. Megatron continues recomputing successive dirty bits until it reaches a cleared dirty bit. When a dirty bit is set, it doesn't need to be enqueued if its predecessor is dirty. This leads to a significant reduction in queue length, and thus speeds up cases where many fields have to be recomputed, such as during initial loads and bulk inserts.

In principle, Megatron could even avoid checking predecessor dirty bits when this optimization is statically known to apply. However, we did not implement this further optimization.

\subsection{Pointer Compression and OM Allocation}
Order maintenance objects have to be allocated every time
  new layout nodes are inserted;
  optimizing that allocation is essential.
We use a hand-written pool allocator;
  in fact, we use separate pools
  for high- and low-level list cells
  to enhance locality.
Since allocations are always the same size,
  and since layout is single-threaded,%
\footnote{As it is in all major browsers.}
  our custom allocator is significantly
  simpler than the system \texttt{malloc}.

The allocator, \texttt{OMPool},
  is shown in Figure~\ref{fig:allocator}.
It is parameterized by \emph{two} types:
  the type of allocated object \texttt{T}
  and the index type \texttt{P} for pointers to allocated objects.
Crucially,
  \texttt{malloc} and \texttt{free} return and consume
  the pointer type \texttt{P} instead of raw pointers.
(The \texttt{addressof} function converts
  the pointer type \texttt{P} to a raw pointer
  so that it can be dereferenced.)
Making \texttt{P} a small integer type,
  like \texttt{uint32} or \texttt{uint16},
  shrinks order maintenance objects,
  which allows more of them fit per cache line,
  improving throughput.
In our implementation, we use 32-bit pointers;
  this conveniently makes the total size
  of an order maintenance object 128 bits,
  meaning that order maintenance objects
  do not split across cache lines.

The actual implementation of \texttt{OMPool} is standard;
  it stores a \texttt{pool} of memory as an \texttt{std::vector},
  in which we ensure sufficient capacity at startup.
Freed elements are placed in a separate \texttt{freed} vector,
  which is preferentially drawn from by \texttt{malloc}.
Because the objects are all the same size,
  there is no fragmentation and \texttt{malloc}/\texttt{free}
  are nearly instantaneous.
Moreover, since Spineless Traversal
  creates Order Maintenance objects in order,
  allocation patterns are extremely favorable,
  with temporally-close nodes often placed nearby in memory.

\begin{figure}
% Newlines below to align figure captions
\begin{Verbatim}[formatcom=\color{gray}, commandchars=+~!]
template<
  typename T,                   // Type of allocated object
  +color~red!~typename P=uint32_t!           +color~black!~// Integer type for "pointers"!
> struct OMPool {
  +color~red!~std::vector<T> pool;!          +color~black!~// Fast, local allocations!
  +color~black!~std::vector<P> freed;         // Rapid reuse minimizes churn!
  P malloc();                   // Straightforward
  +color~black!~void free(P p) { freed.push_back(p); }!
  +color~black!~T* addressof(P p) { return &(pool[p]); }!
}
\end{Verbatim}
\caption{A pooling allocator to reduce cache misses.
  The pointer type $P$ is smaller than a standard pointer,
    allowing order maintenance objects to be smaller
    and thereby fit tightly in a cache line.}
\label{fig:allocator}
\end{figure}

\begin{figure}
\begin{Verbatim}[formatcom=\color{gray}, commandchars={+[]}]
Label lpl = l.parent->label, rpl = r.parent->label;
Label ll = l.label, rl = r.label; uint64_t result;
asm volatile(
  "+color[black][xor   %%rbx,  %%rbx]    +color[gray][\n"]
  "+color[black][cmp   %1,     %2]       +color[gray][\n"]
  "+color[black][seta  %%bl]             +color[gray][\n"]
  "+color[black][xor   %%rax,  %%rax]    +color[gray][\n"]
  "+color[black][cmp   %3,     %4]       +color[gray][\n"]
  "+color[black][seta  %%al]             +color[gray][\n"]
  "+color[red][cmove %%rbx,  %%rax]    +color[gray][\n"]
  : "=&a"(result)
  : "r"(ll), "r"(rl), "r"(lpl), "r"(rpl));
return result;
\end{Verbatim}
\caption{
  The branchless comparison code,
    with the conditional move instruction highlighted.
  Note that, while the assembly snippet
    is seven instructions long,
    the two \texttt{xor} instructions
    are handled by the register renamer,
    and \texttt{cmp}/\texttt{seta} is fused on recent Intel/AMD CPUs.
  The assembly snippet thus executes in three cycles;
    even adding the label loads,
    comparison typically takes only five cycles.
}
\label{fig:compare}
\end{figure}

\subsection{Branchless Order Maintenance Comparison}

By using a binary heap, which has great cache locality,
priority queue pushes and pops spend basically all of their time
  comparing order maintenance objects.
Order maintenance objects are small and,
  thanks to our allocator, typically in cache.
However, order maintenance object comparison has two cases
  (same or different second-level lists)
  and the pipeline stall from the conditional
  ends up being a bottleneck.
Moreover, the branch predictor does not help much,
  because (thanks to the priority queue)
  the comparison is unpredictable.
We therefore implemented a branchless comparison function,
  relying on inline assembly \texttt{cmov} instructions,
  shown in \Cref{fig:compare};
  it executes in about five cycles of latency.
Assembly makes the implementation non-portable,
  but this function is critical for performance
  and we weren't able to make the C++ compiler
  generate comparably-fast code, despite several attempts.
If portability is a concern, we also provided a branchless version implemented using bit operations.

\subsection{Attempted, Failed Optimizations}

A number of attempted optimizations
  failed to improve performance:
\begin{itemize}
\item A hybrid of Double Dirty Bit and Spineless Traversal,
    using a summary bit for subtree dirty bit propagation
    but the priority queue for more distant jumps.
  We were unable to make switching between the two modes
    efficient enough to be competitive.
\item A splay tree instead of a min-heap.
  Slower, likely due to worse cache locality.
\item A red-black tree instead of a min-heap. Also slower.
\item A 1-based array in the min-heap, to speed up index manipulation.
  Slower.
\item 16-bit \texttt{OMPool} pointers/OM labels.
  No faster than 32 bits, and more rebalancings needed.
  We suspect that this is because 16-bit pointers/labels
  only shrink OM cells from 16 to 8 bytes,
  but load ports on modern CPUs already load 16 bytes at a time.
\item Pointer tagging to make priority queue elements smaller.
  No improvement.
\item Splitting OM \texttt{left}/\texttt{right} pointers
  from the \texttt{label} and \texttt{parent}/\texttt{children} pointers, to improve cache locality. No improvement.
\item Deallocating OM objects when the corresponding tree node is deleted, to increase cache locality. A slight slow-down.
\end{itemize}