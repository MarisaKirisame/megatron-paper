\section{Micro-Optimizations}
\label{sec:opt}

Maximizing Spinless Traversal's advantage
  over Double Dirty Bit
  requires careful micro-optimizations
  to improve cache locality,
  reduce memory traffic,
  and alleviate branch mispredictions.
Unlike the optimization passes of the Megatron compiler,
  which apply to all invalidation algorithms,
  these optimizations are implemented
  in the Spineless Traversal invalidation algorithm
  and only affect it.

\subsection{Pointer Compression and OM Allocation}

Order maintenance objects have to be allocated every time
  new layout nodes are inserted;
  optimizing that allocation is essential.
We use a hand-written pool allocator;
  in fact, we use separate pools
  for high- and low-level list cells
  to enhance locality.
Since allocation are always the same size,
  and since layout is single-threaded,%
\footnote{As it is in all major browsers.}
  our custom allocator is significantly
  simpler than the system \texttt{malloc}.

The allocator, \texttt{OMPool},
  is shown in Figure~\ref{fig:allocator}.
It is parameterized by \emph{two} types:
  the type of allocated object \texttt{T}
  and the index type \texttt{P} for pointers to allocated objects.
Crucially,
  \texttt{malloc} and \texttt{free} return and consume
  the pointer type \texttt{P} instead of raw pointers.
(And \texttt{addressof} function converts
  the pointer type \texttt{P} to a raw pointer
  so that it can be dereferenced.)
By making \texttt{P} a small integer type,
  like \texttt{uint32} or \texttt{uint16},
  order maintenance objects become smaller
  and more of them fit per cache line,
  which in turn improves throughput.
In our implementation, we use 32-bit pointers;
  this conveniently makes the total size
  of an order maintenance object 128 bits,
  meaning that no order maintenance objects split across two cache lines.

The actual implementation of \texttt{OMPool} is standard;
  it stores a \texttt{pool} of memory as a \texttt{std::vector},
  in which we ensure sufficient capacity at startup.
Freed elements are placed in a separate \texttt{freed} vector,
  which is preferentially drawn from by \texttt{malloc}.
Because the objects are all the same size,
  there is no fragmentation and \texttt{malloc}/\texttt{free}
  are nearly instantaneous.
Moreover, since Spineless Traversal
  creates Order Maintenance objects in order,
  allocation patterns are extremely favorable,
  with temporally-close nodes often placed nearby in memory.

\begin{figure}
% Newlines below to align figure captions
\begin{Verbatim}[formatcom=\color{gray}, commandchars={+[]}]
template<
  typename T,                   // Type of allocated object
  +color[red][typename P=uint32_t]           +color[black][// Integer type for "pointers"]
> struct OMPool {
  +color[red][std::vector<T> pool;]          +color[black][// Fast, local allocations]
  +color[black][std::vector<P> freed;         // Rapid reuse minimizes churn]
  P malloc();                   // Straightforward
  +color[black][void free(P p) { freed.push_back(p); }]
  +color[black][T* addressof(P p) { return &(pool[p]); }]
}
\end{Verbatim}
\caption{A pooling allocator to reduce cache misses.
  The pointer type $P$ is smaller than a standard pointer,
    allowing order maintenance objects to be smaller
    and thereby fit tightly in a cache line.}
\label{fig:allocator}
\end{figure}

\begin{figure}
\begin{Verbatim}[formatcom=\color{gray}, commandchars={+[]}]
Label lpl = l.parent->label, rpl = r.parent->label;
Label ll = l.label, rl = r.label; uint64_t result;
asm volatile(
  "+color[black][xor   %%rbx,  %%rbx]    +color[gray][\n"]
  "+color[black][cmp   %1,     %2]       +color[gray][\n"]
  "+color[black][seta  %%bl]             +color[gray][\n"]
  "+color[black][xor   %%rax,  %%rax]    +color[gray][\n"]
  "+color[black][cmp   %3,     %4]       +color[gray][\n"]
  "+color[black][seta  %%al]             +color[gray][\n"]
  "+color[red][cmove %%rbx,  %%rax]    +color[gray][\n"]
  : "=&a"(result)
  : "r"(ll), "r"(rl), "r"(lpl), "r"(rpl));
return result;
\end{Verbatim}
\caption{
  The branchless comparison code,
    with the conditional move instruction highlighted.
  Note that, while the assembly snippet
    is seven instructions long,
    the two \texttt{xor} instructions
    are handled by the register renamer,
    and \texttt{cmp}/\texttt{seta} is fused on recent Intel/AMD CPUs.
  The assembly snippet thus executes in three cycles;
    even adding the label loads,
    comparison typically takes only five cycles.
}
\label{fig:compare}
\end{figure}

\subsection{Branchless Order Maintenance Comparison}

Priority queue pushes and pops spend basically all of their time
  comparing order maintenance objects.
Order maintenance objects are small and,
  thanks to our allocator, typically cached.
However, order maintenance object comparison has two cases
  (same or different second-level lists)
  and the bottleneck ends up being the pipeline stall
  induced by the conditional.
The branch predictor does not help much,
  because (thanks to the priority queue) the comparison is unpredictable.
We therefore implemented a branchless comparison function,
  relying on inline assembly \texttt{cmov} instructions,
  shown in \Cref{fig:compare};
  it executes in about five cycles of latency.
Assembly makes the implementation non-portable,
  but this function is critical for performance
  and we weren't able to make the C++ compiler
  generate comparably-fast code, despite several attempts.

\subsection{Attempted, Failed Optimizations}

A number of attempted optimizations
  failed to improve performance:
\begin{itemize}
\item A hybrid of Double Dirty Bit and Spineless Traversal,
    using a summary bit for subtree dirty bit propagation
    but the priority queue for more distant jumps.
  We were unable to make switching between the two modes
    efficient enough to be competitive.
\item A splay tree instead of a min-heap.
  Slower, likely due to worse cache locality.
\item A red-black tree instead of a min-heap. Also slower.
\item A 1-based array in the min-heap, to speed up index manipulation.
  Slower.
\item 16-bit \texttt{OMPool} pointers/OM labels.
  No faster than 32 bits, and more rebalancings needed.
  We suspect that this is because 16-bit pointers/labels
  only shrink OM cells from 16 to 8 bytes,
  but load ports on modern CPUs already load 16 bytes at a time.
\item Pointer tagging to make priority queue elements smaller.
  No faster.
\item Splitting OM \texttt{left}/\texttt{right} pointers
  from the \texttt{label} and \texttt{parent}/\texttt{children} pointers, to improve cache locality. No improvement.
\item Deallocating OM objects when the corresponding tree node is deleted, to increase cache locality. No improvement.
\end{itemize}