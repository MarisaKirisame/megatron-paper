
\section{Introduction}

Latency is a major concern for modern web rendering engines
  such as those used in Chrome, Safari, and Firefox.
A rendering engine
  must redraw the page 60 times per second
  to guarantee smooth animations, fluid interactions, and prompt responses
  as users browse and interact with web pages.
When this frame rate cannot be met,
  the user experiences lag and may be forced to use another web application, browser, or device.
Moreover, demand for low-latency rendering is only increasing
  with modern 120\,Hz displays.

Layout is a key driver of web rendering latency.
Layout means calculating the size and position
  of each element of the web page,
  after which the page can be rendered into pixels on the screen.
Every time the user interacts with the web page
  by hovering over an element,
  receiving updated data,
  or even observing an animation,
  the web page, and thus
  the tree of HTML elements that represent it in memory,
  changes.
To show the updated page to the user,
  the page must then be re-laid-out.
Since the browser has many tasks besides layout,
  such as JavaScript execution,
  this re-lay-out step must be completed in a sub-millisecond budget 
  in order to meet the 60 frame-per-second goal.
On such a tight budget, every cycle counts!

\paragraph{Incrementalization}
The key optimization that makes this possible
  is \emph{incrementalization}.
When an element on the page changes,
  the browser \emph{marks} 
  all dependency of that element as dirty.
Then, when the next frame must be drawn,
  the layout algorithm traverses the node tree
  to find and re-lay-out only the dirty nodes.
In the process it might need to mark additional nodes dirty,
  but typically---in, say, animations or interactions---%
  few layout nodes are ultimately affected.
In these cases, searching the tree
  for dirty elements is the bottleneck,
  especially since every node access
  is likely to incur a cache miss.

To address this issue,
  the state-of-the-art ``Double Dirty Bit'' algorithm
  adds summary bits to each node
  to avoid searching subtrees without any dirty nodes.
While this reduces the search time,
  this algorithm still has to traverse the tree,
  starting from the root, to find dirty nodes.
This means it must access not only the actually-dirty nodes
  but many extra ``auxiliary nodes``:
  the root node, every node between it and a dirty node,
  and all children of that path.
On large pages, there can be far more auxiliary nodes
  than actually-dirty nodes,
  especially for the most latency-sensitive interactions.
Since each node access introduces its own cache miss,
  simply traversing these auxiliary nodes
  (just checking their summary bits)
  stall the layout algorithm for hundreds of microseconds.
Indeed, this problem has been widely observed in practice.
For example,
  Google's widely-used web performance debugging tool, Lighthouse,
  uses tree depth and maximum children count as performance metrics,
  precisely because these parameters
  determine the number of auxiliary nodes.

\paragraph{Spineless Traversal}

We introduce \textit{Spineless Traversal}:
  a new, faster algorithm for incremental layout.
Unlike the standard Double Dirty Bit algorithm,
  Spineless Traversal accesses only dirty nodes, not auxiliary nodes,
  and therefore reduces cache misses.
To do so, Spineless Traversal
  stores the set of dirtied nodes in a priority queue
  and jumps directly from one dirty node to the next,
  with no auxiliary nodes in between.

The key to making this work is maintaining the correct traversal order. 
Recomputing a single field on a single node mark all fields that depend on it,
  and the set of transitive dependencies is complex.
Fields must therefore be recomputed in a specific order,
  and Spineless Traversal must respect that order
  as it jumps from node to node.
Spineless Traversal enforces this
  using an \emph{order maintenance} data structure.
Order maintenance assigns a logical time to each field on each node,
  and provides an efficient way to check which logical time comes first.
The priority queue then uses this order
  to recompute node fields in logical time order.

\paragraph{Evaluation}

We implement Spineless Traversal
  in a new DSL for browser layout algorithms, Megatron.
To match the extremely tight latency budgets of modern browsers,
  Megatron uses a variety of low-level optimizations,
  including unboxing, a custom allocator, pointer compression,
  and an optimized branchless implementation
  of order-maintenance comparison.
We implement a fragment of web layout in Megatron,
  including line breaking, flex-box, intrinsic sizes,
  and other complex features.
We then evaluate both the double dirty bit
  and spineless traversal algorithms, using this implementation,
  on \TotalDiffCount incremental layouts
  of \TotalTraceCount real-world web pages,
  including Twitter, Discord, Github, and Lichess.

Spineless Traversal is \DBPQoverhead times faster on average.
Moreover, these speedups are concentrated in
  the most latency-critical changes
  with the fewest dirty nodes:
  on the \PctSmall of benchmarks
  where fewer than 1\% of fields are recomputed,
  spineless traversal achieves a speedup
  of \MeanSpeedupSmall or more.
Spineless Traversal is only slower than the double dirty bit algorithm
  on \PctSlower of benchmarks,
  with most of these representing less-latency-critical events
  like page loads or navigations.