\section{MegaTron}

To evaluate Spineless Traversal,
  we developed an attribute grammar engine called Megatron
  which implements the DSL of \Cref{fig:dsl}.

\subsection{Compiler Interface}
\label{sec:compiler}

The Megatron compiler
  parses layout rules and schedules
  and compiles them to a fast incremental C++ layout algorithm.
Concretely, the output of Megatron
  is a \texttt{recompute} function,
  which is passed a node and a field name
  and which recomputes that field on that node,
  setting dirty bits for any dependent fields.
To produce this output, the compiler
  analyzes the static dependencies of each field assignment,
  synthesizes dirty bit propagation code,
  and generates packed, cache-friendly data structures.

To evaluate performance,
  we link the generated \texttt{recompute} function
  to a driver program that parses and replays web layout traces
  captured from a real web browser.
The trace is structured as a set of frames,
  which each contain some number of modification actions,
  after which incremental layout is performed.
The execution time of all of these steps is measured
  at very high precision using \texttt{rdtsc}.
Specifically, the driver program
  separates ``overhead'' time---%
  including setting dirty or summary bits,
  allocating OM nodes, and pushing to the priority queue---%
  from ``evaluation'' time,
  which includes the field recomputation itself.%
\footnote{
  Other parts of the driver program,
    like reading and parsing traces, are not measured
    because they are the same for all layout algorithms.}
Evaluation time is nearly identical
  across different invalidation algorithms,
  but overhead time differs dramatically, as expected.

\begin{figure}[tbp]
\begin{verbatim}
val dirty : node -> field_name -> unit
val clean : root : node -> recompute : (node -> field_name -> unit) -> unit
\end{verbatim}
\caption{
  The ``invalidation traversal'' API.
  The \texttt{dirty} method sets the dirty bit
    corresponding to a given field on a given node,
    while the \texttt{clean} function
    invokes the \texttt{recompute} function
    on every dirty field in the layout tree.
  In the actual implementation,
    the node and unit types are staged,
    but the \texttt{field_name} isn't,
    to maximize the performance of the generated code.}
\label{fig:traversal-api}
\end{figure}

To allow for a head-to-head comparison
  between Double Dirty Bit and Spineless Traversal,
  the driver program is parameterized over
  a simple ``invalidation traversal'' API.
It has two key methods:
  a \texttt{dirty} method
  that marks a node's field for later re-computation
  and a \texttt{clean} method
  that invokes \texttt{recompute} on all dirty fields.
Signatures for both functions are shown
  in \Cref{fig:traversal-api};
  there are also boilerplate methods
  to initialize data structures.
The \texttt{dirty} method is called
  both when modifying the layout tree
  and also from the \texttt{recompute} function itself,
  to dirty dependents when a field's value changes.
Three invalidation traversals are available in Megatron:
  a naive traversal that visits each node,
  Double Dirty Bit, and Spineless Traversal.

\subsection{Dependent Synthesis}

The most critical responsibility of the compiler
  is synthesizing code to sets dependent dirty bits
  every time a layout field is modified.
Megatron follows the standard approach from prior work.
In short, for every field $U$,
  the compiler identifies all fields on all nodes
  that are affected when the value of $U$ changes,
  and generates code to set their dirty bits
  by calling the \texttt{dirty} method.
To identify affected fields,
  the compiler examines every \emph{other} field assignment
  $\mathsf{self}.V \gets T$ in the layout rules.
For every field access $N.U$ in $T$,
  we know that $\mathsf{self}.V$ depends on $N.U$,
  meaning that any changes to $\mathsf{self}.U$
  must mark $N^{-1}.V$ as dirty;
  here $N^{-1}$ inverts the relation $N$ by
  flipping \textsf{next} and \textsf{previous},
  mapping \textsf{first} and \textsf{last} to \textsf{parent},
  and mapping \textsf{parent} to all children.

Insertions and deletions also set dirty bits.
Inserting a node changes the meaning
  of the \textsf{prev} and \textsf{next} pointers
  for its siblings, and possibly
  the \textsf{first} and \textsf{last} pointers
  for its parent;
  deleting a node does the same.
This means that inserting or deleting a node
  must dirty every field computation
  that reads \emph{any} field from those pointers,
  and also any field that uses $\mathsf{N?}$ expressions.
In Megatron, the analysis we perform is flow-insensitive;
  a real-world implementation might instead use
  a flow-sensitive analysis to dirty fewer nodes,
  but the engineering trade-offs are more challenging
  and we felt that the focus of this paper---%
  the invalidation traversal---%
  would have similar impact in either case.

As an example, consider the layout rules in \Cref{fig:layout-simple}:
\begin{enumerate}
\item The $W$ field depends on \textsf{parent?} and $\mathsf{parent}.W$.
As the \textsf{parent} of a node cannot change,
  the only 'real' dependency is on $\mathsf{parent}.W$.
Thus, setting $\mathsf{self}.W$ must dirty $\mathsf{parent}^{-1}.W$,
  meaning that after changing $\mathsf{self}.W$
  we must dirty all children's $W$ fields.
\item The $H$ field depends on $\mathsf{last}?$, $\mathsf{last}.HA$,
  and $\mathsf{self}.\text{attribute}[\mathsf{height}]$,
  and the $\mathsf{last}?$ dependency is subsumed by
  the $\mathsf{last}.HA$ dependency.
Taking inverses, modifying $\mathsf{self}.\text{attribute}[\mathsf{height}]$
  must dirty $\mathsf{self}.H$ and,
  if the node is a last child,
  modifying $\mathsf{self}.HA$ or inserting/deleting a node
  must dirty $\mathsf{parent}.H$.
\item The $HA$ field depends on $\mathsf{prev}?$, $\mathsf{prev}.HA$, 
  and $\mathsf{self}.H$, with the $\mathsf{prev}?$ dependency subsumed.
Taking inverses, 
  modifying $\mathsf{self}.H$ must dirty $\mathsf{self}.HA$%
\footnote{
As described later, field packing would likely assign
  the $H$ and $HA$ fields the same dirty bit;
  in this case, modifying $\mathsf{self}.H$
  need not dirty $\mathsf{self}.HA$, since it is already dirty.
}
  and,
  if the node is not a last child,
  modifying $\mathsf{self}.HA$ or inserting/deleting nodes
  must dirty $\mathsf{next}.HA$.
\end{enumerate}

These modification rules, gathered from analyzing each field access,
  are grouped by which field is being modified
  and then injected into the relevant case
  of the \texttt{recompute} function.
For example, the code to recompute $HA$
  would check whether or not the node is a last child
  and dirty either $\mathsf{parent}.H$ (if so)
  or $\mathsf{next}.HA$ (if not).
Additionally,
  Megatron makes sure to only dirty any given field
  once per field recomputation.
For example, if a field $N.V$ is used twice in an expression,
  or if two different accesses $\mathsf{first}.V$ and $\mathsf{last}.V$
  have the same inverse, the field is only dirtied once.
This deduplication is especially challenging
  in the case of $N?$ expressions,
  since whether or not a field is dirtied can depend
  on whether the node is the first or last child of its parent.
This deduplication is complex but, ultimately,
  possible to perform statically,
  which is critical to ensure maximal performance.

To ensure our implementation is correct,
  the compiler can also generate a from-scratch layout function
  which does not use dirty bits at all
  and instead recomputes the entire layout from scratch.
This was extremely valuable during development
  and gives us confidence that our invalidation algorithm is correct.

\subsection{Code Generation}

\begin{figure}[bt]
\begin{verbatim}
val evaluate : term -> env : (variable -> value) -> value
val evaluate_staged : term -> env : (variable -> value code) -> value code
\end{verbatim}
\caption{
  Converting an interpreter into a compiler by staging.
  Terms and variable names are static so are not staged;
    the initial environment and final outputs, however,
    are staged by transformation into an IR.}
\label{fig:stage}
\end{figure}

Megatron's code generation has three key steps:
  generating the node data structure,
  generating the field value computations,
  and generating the top-level \texttt{recompute} function.
To simplify code generation and optimization,
  the compiler is organized as a staged interpreter~\cite{MetaOCaml}.
That is, we first implemented an interpreter,
  and then added staging annotations that generate a custom IR;
  \Cref{fig:stage} summarizes the approach.
The custom IR is then converted to C++
  using traditional compiler optimizations.
This approach was critical,
  as we found compiler correctness challenging:
  flaws in the dependency analysis could have unintuitive,
  long-distance consequences on program behavior
  that were difficult to distinguish from incorrect optimizations.
The staging step allowed for much easier debugging.

\paragraph{Data structures}
To generate efficient node data structures,
  Megatron type-checks all fields, attributes, and properties
  using Hindley-Milner type inference~\cite{HM}
  and uses appropriate unboxed C++ member variables
  to store the relevant values.
Importantly, this means a single node and all its fields
  are contiguous in memory
  (as it would be a real web browser)
  with a minimum of pointers
  (beyond the standard parent/first/last/next/previous pointers
  to other layout nodes),
  with field access compiled to a memory offset.
All string values (like the keyword values for \texttt{display})
  are interned and represented in C++ as
  a single \texttt{enum} type,
  meaning that no string allocation or comparison
  is performed at runtime.
Discriminated unions are used for fields with units
  (for example $\text{property}[\mathsf{length}]$
    can be an absolute length or a percentage).
Dirty bits, summary bits, and timestamps
  are placed adjacent to the fields they cover.

To reduce overhead as much as possible,
  Megatron \emph{packs} multiple co-computed fields,
  covering them with a single dirty bit.
Packed fields are laid out adjacent in memory
  because they are written to, and often read from,
  in rapid succession.
While more sophisticated techniques exist~\cite{yufeng-2},
  Megatron simply uses two dirty bits
  for each in the layout rules:
  one for the pre-order-computed fields
  and one for post-order-computed fields.
The intuition is that this minimizes
  the number of unique dirty bits
  set during dirty bit propagation
  and reduces the size of the priority queue
  and the number of order maintenance objects needed.
Field packing has complex trade-offs
  not implemented in Megatron
  but common in real browsers;
  however, Spineless Traversal would work
  with any field packing.

\paragraph{Field computations}
To generate efficient field computations,
  the compiler applies strength reduction and dead code elimination,
  implemented as a simple walk over the AST.
The bottom line is that the compiled code is
  long but readable, idiomatic, and fairly low-level C++ code
  with no allocation, hash, or string operations.
The C++ compiler is then invoked, which performs its own optimizations.
This is critical because it means that,
  like in a real web browser,
  computing fields is very fast
  and the cache misses from finding dirty fields
  are a measurable fraction of the runtime.
The \texttt{recompute} function uses its field name argument
  to determine which field computations to run.
For Spineless Traversal,
  the field name is an \texttt{enum}
  and is used as an index into a jump table
  to perform the relevant recomputation;
  the special $(p, r)$ elements are included
  in the same \texttt{enum}.
For Double Dirty Bit, the field name is known
  entirely statically, and our compiler
  simply outputs the relevant code.